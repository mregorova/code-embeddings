# code-embeddings
My NLP pet-project, which is used for code embeddings research in programmimg.

# To start with: выбор датасета
В постановке задачи был рекомендован датасет CodeSearchNet, однако за основу я взяла найденный на Codeforces датасет Code_contests от Deepmind. Для представленной задачи его преимущество в том, что код (который является решением задач на cf) представлен сразу на нескольких языках программирования, что позволяет оценить качество модели путем сравнения эмбеддингов, полученных на основе разного синтаксиса предложений.

Множество полученных решений выглядит так:  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/solutions1.png" width="500" height="360">

# Выбор языковой модели
Прежде всего, стоит рассмотреть версии моделей, предназначенные для работы с кодом, такие как codeBERT и codeT5, так как они более специализированы, чем сами по себе BERT и T5. Также для работы с кодом используются такие модели, как OpenAI, CodeGeeX, code2vec и многие другие. 
Основное исследование и сравнение метрик было проведено при помощи codeBERT.

# Получение эмбеддингов и анализ качества
Написав функцию embed для извлечения эмбеддингов, получаем соответствующие вектора в формате torch. Это потребуется для применения метрик сравнения, о которых будет рассказано далее. 
Итак, возьмём случайным образом некоторую пару и получим эмбеддинги кода. Как понять, что модель действительно выделяет оттуда смысл?
Для решения этой задачи был использован подход, похожий на принцип работы сиамских сетей - в качестве меры похожести мы используем косинусное расстояние между эмбеддингами кода. При помощи написания функции cos_difference со встроенными инструментами pytorch выводим расстояния 0.009 ± 0.001 в рамках одной и той же задачи и 0.013 ± 0.001 для двух разных. 
Это позволяет сделать вывод, что модель действительно понимает смысл обрабатываемого кода, однако в силу несущественного различия между одинаковыми и разными заданиями исследуем модель дальше.

Если посчитать косинусное расстояние для большего множества задач в рамках языка Python и сравнить (в том числе при помощи графика ниже), можно сделать вывод, что модель не различает одинаковые и различные задания :(
<img src="https://github.com/mregorova/code-embeddings/blob/main/images/comparison1.png" width="500" height="360">
