# code-embeddings
My NLP pet-project, which is used for code embeddings research in programmimg.

# To start with: выбор датасета
В постановке задачи был рекомендован датасет CodeSearchNet, однако за основу я взяла найденный на Codeforces датасет Code_contests от Deepmind. Для представленной задачи его преимущество в том, что код (который является решением задач на cf) представлен сразу на нескольких языках программирования, что позволяет оценить качество модели путем сравнения эмбеддингов, полученных на основе разного синтаксиса предложений.

Множество полученных решений выглядит так:  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/solutions1.png" width="500" height="360">

# Выбор языковой модели
Прежде всего, стоит рассмотреть версии моделей, предназначенные для работы с кодом, такие как codeBERT и codeT5, так как они более специализированы, чем сами по себе BERT и T5. Также для работы с кодом используются такие модели, как OpenAI, CodeGeeX, code2vec и многие другие. 
Основное исследование и сравнение метрик было проведено при помощи codeBERT.

# Получение эмбеддингов и анализ качества
Написав функцию embed для извлечения эмбеддингов, получаем соответствующие вектора в формате torch. Это потребуется для применения метрик сравнения, о которых будет рассказано далее. 
Итак, возьмём случайным образом некоторую пару и получим эмбеддинги кода. Как понять, что модель действительно выделяет оттуда смысл?
Для решения этой задачи был использован подход, похожий на принцип работы сиамских сетей - в качестве меры похожести мы используем косинусное расстояние между эмбеддингами кода. При помощи написания функции cos_difference со встроенными инструментами pytorch выводим расстояния 0.009 ± 0.001 в рамках одной и той же задачи и 0.013 ± 0.001 для двух разных. 
Это позволяет сделать вывод, что модель действительно понимает смысл обрабатываемого кода, однако в силу несущественного различия между одинаковыми и разными заданиями исследуем модель дальше.

Если посчитать косинусное расстояние для большего множества задач в рамках языка Python и сравнить (в том числе при помощи графика ниже), можно сделать вывод, что модель не различает одинаковые и различные задания :(  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/comparison1.png" width="500" height="360">

Продолжим эксперимент и сравним множество косинусных расстояний между разными заданиями с множеством расстояний между решениями на разных языках программирования (например, С++ и Python). Полученный график демонстрирует, что все же модель способна разделять некоторые смысловые характеристики решений.  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/comparison2.png" width="500" height="360">

# Что ещё можно выделить?
Попробуем исследовать связь описания задачи и её решения. Для этого вычислим эмбеддинги не только для самих решения, но и для описаний. Полученная точечная диаграмма демонстрирует, что эти множества модель разделяет хорошо. Но связаны ли они как-то между собой?  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/arrays.png" width="500" height="360">

Выберем несколько точек, построим соответствие между ними, иии... Связь действительно есть! Её можно интерпретировать различными способами, но на картинке чётко наблюдается несколько множеств линий с практически одинаковым углом наклона, что свидетельствует о прочной связи между описанием и решением на Codeforces, то есть существует так называемый "вектор решения задачи".
<img src="https://github.com/mregorova/code-embeddings/blob/main/images/arrays_connection.png" width="500" height="360">

# Выводы и планы на будущее
