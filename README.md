# code-embeddings
My NLP pet-project, which is used for code embeddings research in programmimg.

# To start with: выбор датасета
В постановке задачи был рекомендован датасет CodeSearchNet, однако за основу я взяла найденный на Codeforces датасет Code_contests от Deepmind. Для представленной задачи его преимущество в том, что код (который является решением задач на cf) представлен сразу на нескольких языках программирования, что позволяет оценить качество модели путем сравнения эмбеддингов, полученных на основе разного синтаксиса предложений.

Множество полученных решений выглядит так:  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/solutions1.png" width="500" height="360">

# Выбор языковой модели
Прежде всего, стоит рассмотреть версии моделей, предназначенные для работы с кодом, такие как codeBERT и codeT5, так как они более специализированы, чем сами по себе BERT и T5. Также для работы с кодом используются такие модели, как OpenAI, CodeGeeX, code2vec и многие другие. 
Основное исследование и сравнение метрик было проведено при помощи codeBERT.

# Получение эмбеддингов и анализ качества
Написав функцию embed для извлечения эмбеддингов, получаем соответствующие вектора в формате torch. Это потребуется для применения метрик сравнения, о которых будет рассказано далее. 
```
def embed(texts: [str], batch_size: int, model, tokenizer: transformers.PreTrainedTokenizerFast, max_len: int = 500) -> [torch.Tensor]:
  embeddings = []
  for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    encoded = tokenizer(batch, padding=True, return_tensors='pt')
    with torch.no_grad():
      outputs = model(input_ids=encoded.input_ids[:,:max_len], attention_mask=encoded.attention_mask[:,:max_len])
    embeddings.append(outputs.last_hidden_state[:,0,:])
  
  return torch.cat(embeddings)
```
Итак, возьмём случайным образом некоторую пару и получим эмбеддинги кода. Как понять, что модель действительно выделяет оттуда смысл?
Для решения этой задачи был использован подход, похожий на принцип работы сиамских сетей - в качестве меры похожести мы используем косинусное расстояние между эмбеддингами кода. При помощи написания функции cos_difference со встроенными инструментами pytorch получим расстояния 0.009 ± 0.001 в рамках одной и той же задачи и 0.013 ± 0.001 для двух разных. 
Это позволяет сделать вывод, что модель действительно понимает смысл обрабатываемого кода, однако в силу несущественного различия между одинаковыми и разными заданиями исследуем модель дальше.

Если посчитать косинусное расстояние для большего множества задач в рамках языка Python и сравнить (в том числе при помощи графика ниже), можно сделать вывод, что модель не различает одинаковые и различные задания :(  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/comparison1.png" width="500" height="360">

Продолжим эксперимент и сравним множество косинусных расстояний между разными заданиями с множеством расстояний между решениями на разных языках программирования (например, С++ и Python). Полученный график демонстрирует, что все же модель способна разделять некоторые смысловые характеристики решений.  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/comparison2.png" width="500" height="360">

# Что ещё можно выделить?
Попробуем исследовать связь описания задачи и её решения. Для этого вычислим эмбеддинги не только для самих решения, но и для описаний. Полученная точечная диаграмма демонстрирует, что эти множества модель разделяет хорошо. Но связаны ли они как-то между собой?  

<img src="https://github.com/mregorova/code-embeddings/blob/main/images/arrays.png" width="500" height="360">

Выберем несколько точек, построим соответствие между ними, иии... Связь действительно есть! Её можно интерпретировать различными способами, но на картинке чётко наблюдается несколько множеств линий с практически одинаковым углом наклона (условно их можно разбить на три кластера), что свидетельствует о прочной связи между описанием и решением на Codeforces, то есть существует так называемый "вектор решения задачи".
<img src="https://github.com/mregorova/code-embeddings/blob/main/images/arrays_connection.png" width="500" height="360">

# Выводы и планы на будущее
Для сравнения качества также была использована модель от YandexGPT, но она не выделяет смыслов в тексте и не способна находить связей между описанием и решением задачи. 

Также была опробована sentence-transformers модель, являющаяся одной из версий BERT - codebert-base-cd-ft от mchochlov. Она работает ощутимо хуже предыдущей, о чём говорят показатели косинусного расстояния 0.1091 ± 0.0039 и 0.1002 ± 0.0036 соответственно.
Почему же именно модель codeBERT работает лучше? Идея с энкодером трансформеров не является новой, но особенности этой модели именно в особом подходе к обработке данных при помощи внимания и масок MLM, а также характеристика bidirectional в самом названии.

Немного о Papers: в будущем хотелось бы детальнее изучить модели на практике, представленные на этом сайте, и понять, какие метрики дают им такой результат. Пока что на примере генерации кода видно, что даже идея с деревьями в двух версиях GPT смещает codeBERT к краю лидерства: https://paperswithcode.com/task/code-generation

Подводя итоги, можем подтвердить гипотезу, что на данный момент языковые модели недостаточно хорошо умеют выделять смысловые эмбеддинги. Звучит как отличный повод продолжить исследование метрик и эксперименты с моделями или даже написать собственную, чтобы занять почётное место в Papers leaderboard :)
